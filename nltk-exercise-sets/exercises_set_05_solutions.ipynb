{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scskalicky/LING-226-vuw/blob/main/nltk-exercise-sets/exercises_set_05_solutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Kb8uT8UV-V-"
      },
      "source": [
        "# Exercise set 05"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zrbjuen42OPj"
      },
      "source": [
        "import nltk\n",
        "nltk.download(['punkt', 'book', 'averaged_perceptron_tagger'])\n",
        "from nltk.corpus import brown"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjslY2yHmCJ0"
      },
      "source": [
        "1. ☼ Search the web for \"spoof newspaper headlines\", to find such gems as: British Left Waffles on Falkland Islands, and Juvenile Court to Try Shooting Defendant. Manually tag these headlines to see if knowledge of the part-of-speech tags removes the ambiguity.\n",
        "\n",
        "NOTE: Here are two websites I found with some examples: [one](https://www.departments.bucknell.edu/linguistics/synhead.html), [two](http://www.fun-with-words.com/ambiguous_headlines.html).\n",
        "\n",
        "Use the `nltk.tag.str2tuple` method for making these tags. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCye7S-Aril_"
      },
      "source": [
        "sentv1 = 'EYE/NOUN DROPS/VERB OFF/PREP SHELF/NOUN'\n",
        "\n",
        "sentv2 = 'EYE/NOUN DROPS/NOUN OFF/PREP SHELF/NOUN'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jcJuuuOsJ8c"
      },
      "source": [
        "[nltk.tag.str2tuple(w) for w in nltk.word_tokenize(sentv1)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1RV0umXszRM"
      },
      "source": [
        "[nltk.tag.str2tuple(w) for w in nltk.word_tokenize(sentv2)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61h5bPBQ2Sxe"
      },
      "source": [
        "3. ☼ Tokenize and tag the following sentence: `They wind back the clock, while we chase after the wind`. What different pronunciations and parts of speech are involved?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2VttGp5FOL6"
      },
      "source": [
        "# the smart thing to do here would be to use the built-in word_tokenize and pos_tag functions. \n",
        "help(nltk.pos_tag)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SHIh-jFE3Pd"
      },
      "source": [
        "tagged_sent = nltk.pos_tag(nltk.word_tokenize('They wind back the clock, while we chase after the wind.'))\n",
        "tagged_sent\n",
        "\n",
        "# i suppose they want to focus on wind/wind and how pronunciation denotes part of speech?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVdvdnC12TsT"
      },
      "source": [
        "11. ☼ Learn about the affix tagger (type `help(nltk.AffixTagger)`). Train an affix tagger and run it on some new text. Experiment with different settings for the affix length and the minimum word length. Discuss your findings.\n",
        "\n",
        "\n",
        "NOTE: You will need to provide a dictionary of mappings as a model to the affix tagger. Then play with the `affix_length` and `min_stem_length` arguments.\n",
        "\n",
        "Make sure to run it on suffixes as well by using negative values for `affix_length`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejtf8dBH3Le5"
      },
      "source": [
        "help(nltk.AffixTagger)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cwZ1w3r2e_v"
      },
      "source": [
        "# making some dictionaries for prefixes and suffixes.\n",
        "prefixes = {'un':'ADJ', \n",
        "            'wh':\"WH\",\n",
        "            'di': 'VERB'}\n",
        "\n",
        "suffixes = {'ness': 'NOUN', \n",
        "            'tion': 'NOUN', \n",
        "            'sion': 'NOUN', \n",
        "            'ly': 'ADV', \n",
        "            'er': 'NOUN', \n",
        "            'or': 'NOUN',\n",
        "            'able': 'ADJ',\n",
        "            'ible': 'ADJ',\n",
        "            'ine': 'NOUN'}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2EqTgmbH8sc"
      },
      "source": [
        "prefix_tagger = nltk.AffixTagger(model = prefixes, affix_length = 2, min_stem_length = 2)\n",
        "suffix_tagger = nltk.AffixTagger(model = suffixes, affix_length = -4, min_stem_length = 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ADyIONjHdw-"
      },
      "source": [
        "suffix_tags = suffix_tagger.tag(nltk.word_tokenize(open('/content/drive/MyDrive/texts/marine_biologist.txt').read().lower()))\n",
        "[w for w in suffix_tags if w[1] != None]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suMGaPdDKHDI"
      },
      "source": [
        "# clearly a rule based approach leaves much to be desired. \n",
        "prefix_tags = prefix_tagger.tag(nltk.word_tokenize(open('/content/drive/MyDrive/texts/marine_biologist.txt').read().lower()))\n",
        "[w for w in prefix_tags if w[1] != None]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nN58DNt54SfM"
      },
      "source": [
        "12. ☼ Train a bigram tagger with no backoff tagger, and run it on some of the training data. Next, run it on some new data. What happens to the performance of the tagger? Why?\n",
        "\n",
        "NOTE: You will need some tagged data to do this. Brown is probably the easier one. Try training on one genre and apply it to another. Train something on `tagged_sents` from one genre and apply it to `tagged_sents` in another. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrY6-Ht7KjBl"
      },
      "source": [
        "from nltk.corpus import brown\n",
        "\n",
        "brown_news_sentTags = brown.tagged_sents(categories = 'news')\n",
        "news_unigram_tagger = nltk.UnigramTagger(brown.tagged_sents(categories = 'news'))\n",
        "\n",
        "news_bigram_tagger01 = nltk.BigramTagger(brown_news_sentTags)\n",
        "news_bigram_tagger02 = nltk.BigramTagger(brown_news_sentTags, backoff = news_unigram_tagger)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVBHx9CRLvwB"
      },
      "source": [
        "news_bigram_tagger01.evaluate(brown.tagged_sents(categories = 'humor'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6_chqCbNWHd"
      },
      "source": [
        "# Quite the amazing bump in accuracy, for humor at least. \n",
        "news_bigram_tagger02.evaluate(brown.tagged_sents(categories = 'humor'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DB1NJO7J49_s"
      },
      "source": [
        "20. ◑ Write code to search the Brown Corpus for particular words and phrases according to tags, to answer the following questions:\n",
        "- Produce an alphabetically sorted list of the distinct words tagged as `MD`.\n",
        "- Identify words that can be plural nouns or third person singular verbs (e.g. deals, flies).\n",
        "- Identify three-word prepositional phrases of the form IN + DET + NN (eg. in the lab).\n",
        "- What is the ratio of masculine to feminine pronouns?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNRQqA4vNi4o"
      },
      "source": [
        "brown_tagged = brown.tagged_words()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9mmll3XQrZq"
      },
      "source": [
        "# helps to look at all the tags\n",
        "print(sorted(set([w[1] for w in brown_tagged])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b038WQb9Prk5"
      },
      "source": [
        "# Produce an alphabetically sorted list of the distinct words tagged as MD.\n",
        "sorted(set([w for w in brown_tagged if w[1] == 'MD']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQUgaHsORV6D"
      },
      "source": [
        " nltk.help.upenn_tagset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PB6Jv5g7QPVP"
      },
      "source": [
        "# Identify words that can be plural nouns or third person singular verbs (e.g. deals, flies).\n",
        "sorted(set([w for w in brown_tagged if w[1] == 'NNS' or w[1] == 'VBZ']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4P8JRtpVOP8"
      },
      "source": [
        "# Identify three-word prepositional phrases of the form IN + DET + NN (eg. in the lab).\n",
        "# use the example from the chapter of looping through trigrams. \n",
        "three_word_preps = []\n",
        "for (w1, t1), (w2, t2), (w3, t3) in nltk.trigrams(brown_tagged):\n",
        "  if t1 == 'IN' and t2 == 'DT' and t3 == 'NN':\n",
        "    three_word_preps.append((w1, w2, w3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJ230XevV8UU"
      },
      "source": [
        "three_word_preps"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlIuGWpiXGPW"
      },
      "source": [
        "It is probably necessary for students to read this section from Brown corpus description:\n",
        "\n",
        "\n",
        "We first need to define which is which\n",
        "\n",
        "Pronoun tags in Brown start with \"PP\"\n",
        "\n",
        "Through trial and error you might find the right tags, PP$ will have `him` and `her`, PPS will have `he` and `she`. I honestly don't get this exercsise because it's probalby easier to just write a list of pronouns. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HiLAdV5AarO3"
      },
      "source": [
        "print(sorted(set([w[1] for w in brown_tagged if w[1].startswith('PP')])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQL0AKxjXFAm"
      },
      "source": [
        "pronouns = [w for w in brown_tagged if w[1] == 'PP$']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsOjHthgXUkZ"
      },
      "source": [
        "set([w[0].lower() for w in pronouns])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0hToUNzdpqS"
      },
      "source": [
        "male_pronouns = ['he', 'his', 'him']\n",
        "female_pronouns = ['her', 'hers', 'she']\n",
        "\n",
        "brown_male = [w for w in brown_tagged if w[0].lower() in male_pronouns]\n",
        "brown_female = [w for w in brown_tagged if w[0].lower() in female_pronouns]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0O4-gLNUehuA"
      },
      "source": [
        "# I suppose they want to show that there are far more male than female pronouns used\n",
        "# doing a straight up frequency distribution might be more interesting \n",
        "len(brown_female)/len(brown_male)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjb1Ou5Fve29"
      },
      "source": [
        "# i actually don't know how tags is supposed to help here. something to discuss I suppose. \n",
        "pronoun_fd = nltk.FreqDist([w for w in brown.words() if w in male_pronouns or w in female_pronouns])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDdW_z8kvzx4"
      },
      "source": [
        "pronoun_fd.tabulate()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
