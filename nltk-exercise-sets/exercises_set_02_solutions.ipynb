{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNgUwyn5A7s3QYk0UOE3Qre",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scskalicky/LING-226-vuw/blob/main/nltk-exercise-sets/exercises_set_02_solutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tj_td7i7DeKP"
      },
      "source": [
        "# Exercise set 02\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VOgDOzcT7lJ"
      },
      "source": [
        "import nltk\n",
        "nltk.download('book')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OK5jgahugs6"
      },
      "source": [
        "4. ☼ Read in the texts of the State of the Union addresses, using the state_union corpus reader. Count occurrences of men, women, and people in each document. What has happened to the usage of these words over time?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0HbtryMujg8"
      },
      "source": [
        "# your answer here\n",
        "\n",
        "# to solve this we first need to figure out how to represent this over time - we need the year. We can find that in the file id\n",
        "print(nltk.corpus.state_union.fileids())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJEZ7eaBmvc3"
      },
      "source": [
        "# now we can follow the same approach used for the inagural speech\n",
        "# use nltk.ConditionalFreqDist\n",
        "# they use a really long list comprehension which is nested and not in brackets\n",
        "from nltk.corpus import state_union\n",
        "su_cfd = nltk.ConditionalFreqDist(\n",
        "    (target, fileid[:4]) \n",
        "    for fileid in state_union.fileids()\n",
        "    for w in state_union.words(fileid)\n",
        "    for target in ['men', 'women', 'people']\n",
        "    if w.lower().startswith(target))\n",
        "\n",
        "su_cfd.tabulate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVysR1cCm36O"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize = (20, 10))\n",
        "su_cfd.plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJgdJYYGm0Ho"
      },
      "source": [
        "# Here is what the loop is doing. \n",
        "for file in state_union.fileids():\n",
        "  year = file[:4]\n",
        "  for w in state_union.words(file):\n",
        "    if w in ['man', 'woman', 'people']:\n",
        "      print(year, w, end = ' ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdyxdxDXBQNy"
      },
      "source": [
        "7. ☼ According to Strunk and White's *Elements of Style*, the word `however`, used at the start of a sentence, means \"in whatever way\" or \"to whatever extent\", and not \"nevertheless\". They give this example of correct usage: [However you advise him, he will probably do as he thinks best.](http://www.bartleby.com/141/strunk3.html) Use the concordance tool to study actual usage of this word in the various texts we have been considering. See also the LanguageLog posting [\"Fossilized prejudices about 'however'.\"](http://itre.cis.upenn.edu/~myl/languagelog/archives/001913.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQOlkpt7BQ4H"
      },
      "source": [
        "# this one is annoying because you have to remember how to get the text into the right format\n",
        "emma = nltk.Text(nltk.corpus.gutenberg.words('austen-emma.txt'))\n",
        "emma.concordance(\"however\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8_qOdZ5CaYW"
      },
      "source": [
        "# contrast with a different corpus\n",
        "for file in state_union.fileids():\n",
        "  nltk.Text(state_union.words(file)).concordance('however')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ohmMMODEo3F"
      },
      "source": [
        "# as list comprehension\n",
        "print([nltk.Text(state_union.words(i)).concordance('however') for i in state_union.fileids()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IROdBwQ-M05m"
      },
      "source": [
        "15. ◑ Write a program to find all words that occur at least three times in the Brown Corpus.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-d7CTmwM2mx"
      },
      "source": [
        "from nltk.corpus import brown\n",
        "brown_fd = nltk.FreqDist(w.lower() for w in brown.words())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krExpdCWOCx9"
      },
      "source": [
        "brown_df = nltk.FreqDist(brown.words())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3aBlqNCFOMVp"
      },
      "source": [
        "triple_words = [w for w in brown_df.keys() if brown_df[w] > 2]\n",
        "print(len(brown.words()))\n",
        "print(len(triple_words))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mIkcyo3OYfa"
      },
      "source": [
        "16. ◑ Write a program to generate a table of lexical diversity scores (i.e. token/type ratios), as we saw in Chapter 2 Section 1.1. Include the full set of Brown Corpus genres (`nltk.corpus.brown.categories()`). Which genre has the lowest diversity (greatest number of tokens per type)? Is this what you would have expected?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ps0hS5QlOb52"
      },
      "source": [
        "def lexical_diversity(text):\n",
        "    return len(set(text)) / len(text)\n",
        "for genre in nltk.corpus.brown.categories():\n",
        "    print(genre + ': ' + str(lexical_diversity(brown.words(categories = genre))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glx1Cy2cXZ80"
      },
      "source": [
        "19. ◑ Write a program to create a table of word frequencies by genre, like the one given in Chapter 2, Section 1 for modals. Choose your own words and try to find words whose presence (or absence) is typical of a genre. Discuss your findings. \n",
        "\n",
        "NOTE: Use the Brown corpus and its built-in genres.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IyYtosXXbF9"
      },
      "source": [
        "# I can only assume they want us to use the brown corpus because it has the genre categories\n",
        "\n",
        "cfd = nltk.ConditionalFreqDist(\n",
        "  (genre, word)\n",
        "  for genre in brown.categories()\n",
        "  for word in brown.words(categories = genre))\n",
        "\n",
        "genres = ['news', 'religion', 'hobbies', 'science_fiction', 'romance', 'humor']\n",
        "\n",
        "target_words = ['space', 'war', 'fight', 'happy', 'sad', 'man', 'woman', 'child']\n",
        "\n",
        "cfd.tabulate(conditions = genres, samples = target_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbam5O70YHeY"
      },
      "source": [
        "20. ◑ Write a function `word_freq()` that takes a word and the name of a section of the Brown Corpus as arguments, and computes the frequency of the word in that section of the corpus.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xm7HKbHkYKLH"
      },
      "source": [
        "# This is actually a really nice program to make :)\n",
        "def word_freq(word, section):\n",
        "  words = brown.words(categories = section)\n",
        "  fdist = nltk.FreqDist(w.lower() for w in words if w == word)\n",
        "  return fdist\n",
        "\n",
        "word_freq('war', 'romance')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
